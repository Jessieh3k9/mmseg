EncoderDecoder(
  (data_preprocessor): SegDataPreProcessor()
  (backbone): FRNet(
    (dict_module): ModuleDict(
      (conv0): ResidualBlock(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (the_bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (conv1): RecurrentConvNeXtBlock(
        (dwconv): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          )
        )
        (norm): LayerNorm()
        (pwconv1): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (act): GELU(approximate='none')
        (pwconv2): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (drop_path): DropPath(drop_prob=0.400)
      )
      (conv2): RecurrentConvNeXtBlock(
        (dwconv): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          )
        )
        (norm): LayerNorm()
        (pwconv1): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (act): GELU(approximate='none')
        (pwconv2): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (drop_path): DropPath(drop_prob=0.400)
      )
      (conv3): RecurrentConvNeXtBlock(
        (dwconv): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          )
        )
        (norm): LayerNorm()
        (pwconv1): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (act): GELU(approximate='none')
        (pwconv2): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (drop_path): DropPath(drop_prob=0.400)
      )
      (conv4): RecurrentConvNeXtBlock(
        (dwconv): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          )
        )
        (norm): LayerNorm()
        (pwconv1): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (act): GELU(approximate='none')
        (pwconv2): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (drop_path): DropPath(drop_prob=0.400)
      )
      (conv5): RecurrentConvNeXtBlock(
        (dwconv): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          )
        )
        (norm): LayerNorm()
        (pwconv1): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (act): GELU(approximate='none')
        (pwconv2): RecurrentBlock(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (drop_path): DropPath(drop_prob=0.400)
      )
      (final): Sequential(
        (0): Conv2d(32, 12, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), bias=False)
        (1): Sigmoid()
      )
    )
  )
  (decode_head): FCNHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): ModuleList(
      (0): CrossEntropyLoss(avg_non_ignore=False)
      (1): DiceLoss()
    )
    (conv_seg): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (convs): Sequential(
      (0): ConvModule(
        (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activate): ReLU(inplace=True)
      )
    )
    (conv_cat): ConvModule(
      (conv): Conv2d(33, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (activate): ReLU(inplace=True)
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)